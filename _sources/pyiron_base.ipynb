{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# pyiron \nThe integrated development environment (IDE) for computational materials science `pyiron` accelerates the rapid prototyping and up-scaling of simulation protocols. Internally, it consists of two primary components the `pyiron_atomistics` package, which provides the interfaces for atomistic simulations codes and atomistic simulation workflows and the `pyiron_base` package, which defines the job management and data storage interface. The latter is independent of the atomistic scale and addresses the general challenge of coupling simulation codes in reproducible workflows. Simulation codes can be integrated in the `pyiron_base` package by either using existing python bindings or alternatively by writing the input files, executing the simulation code and parsing the output files. The following explanations focus on the `pyiron_base` package as a workflow manager, which constructs simulation workflows by combining `job` objects like building blocks.\n\n## Installation / Setup\nThe `pyiron_base` workflow manager can be installed via the python package index or the conda package manager. While no additional configuration is required to use `pyiron_base` on a workstation, the connection to an high performance computing (HPC) cluster requires some additional configuration. The `.pyiron` configuration file in the users home directory is used to specify the resource directory, which contains the configuration of the queuing system. \n\n## Implementation of a new simulation code\nThe `pyiron_base` workflow manager provides two interfaces to implement new simulation codes or simulation workflows. For simulation codes which already provide a python interface the `wrap_python_function()` function is used to convert any python function into a pyiron job object. In analogy external executables can be wrapped using the `wrap_executable()`. Based on these two functions any executable can be wrapped as `Job` object. By naming the `Job` object the user can easily reload the same calculation at any time. Furthermore, `pyiron_base` internally uses the name to generate a directory for each `Job` object to simplify locating the input and output of a given calculation for debugging: ","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ase.io import write\nfrom adis_tools.parsers import parse_pw\nfrom pyiron_base.project.delayed import draw","metadata":{"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def write_input(input_dict, working_directory=\".\"):\n    filename = os.path.join(working_directory, 'input.pwi')\n    os.makedirs(working_directory, exist_ok=True)\n    write(\n        filename=filename, \n        images=input_dict[\"structure\"], \n        Crystal=True, \n        kpts=input_dict[\"kpts\"], \n        input_data={\n            'calculation': input_dict[\"calculation\"],\n            'occupations': 'smearing',\n            'degauss': input_dict[\"smearing\"],\n        }, \n        pseudopotentials=input_dict[\"pseudopotentials\"],\n        tstress=True, \n        tprnfor=True\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def collect_output(working_directory=\".\"):\n    output = parse_pw(os.path.join(working_directory, 'pwscf.xml'))\n    return {\n        \"structure\": output['ase_structure'],\n        \"energy\": output[\"energy\"],\n        \"volume\": output['ase_structure'].get_volume(),\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Finally, multiple simulation can be combined in a simulation protocol. In this case the optimization of an Aluminium lattice structure, the calculation of an energy volume curve and the plotting of the resulting curve. ","metadata":{}},{"cell_type":"code","source":"def generate_structures(structure, strain_lst): \n    structure_lst = []\n    for strain in strain_lst:\n        structure_strain = structure.copy()\n        structure_strain.set_cell(\n            structure_strain.cell * strain**(1/3), \n            scale_atoms=True\n        )\n        structure_lst.append(structure_strain)\n    return structure_lst","metadata":{"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from ase.build import bulk\nfrom pyiron_base import Project","metadata":{"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"project = Project(\"test\")\nproject.remove_jobs(recursive=True, silently=True)\nstructure = bulk('Al', a=4.05, cubic=True)\npseudopotentials = {\"Al\": \"Al.pbe-n-kjpaw_psl.1.0.0.UPF\"}","metadata":{"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"057dbe2099484caaa40a8bc7e050107a"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Structure optimization \njob_qe_minimize = project.wrap_executable(\n    job_name=\"job_qe_minimize\",\n    write_input_funct=write_input,\n    collect_output_funct=collect_output,\n    input_dict={\n        \"structure\": structure, \n        \"pseudopotentials\": pseudopotentials, \n        \"kpts\": (3, 3, 3),\n        \"calculation\": \"vc-relax\",\n        \"smearing\": 0.02,\n    },\n    executable_str=\"mpirun -np 1 pw.x -in input.pwi > output.pwo\",\n    delayed=True,\n    output_file_lst=[],\n    output_key_lst=[\"structure\"],\n)\n\n# Generate Structures\nnumber_of_strains = 5 \nstructure_lst = project.wrap_python_function(\n    python_function=generate_structures,\n    structure=job_qe_minimize.output.structure, \n    strain_lst=np.linspace(0.9, 1.1, number_of_strains),\n    delayed=True,\n    list_length=number_of_strains,\n)\n\n# Energy Volume Curve \nenergy_lst, volume_lst = [], []\nfor i, structure_strain in enumerate(structure_lst):\n    job_strain = project.wrap_executable(\n        job_name=\"job_strain_\" + str(i),\n        write_input_funct=write_input,\n        collect_output_funct=collect_output,\n        input_dict={\n            \"structure\": structure_strain, \n            \"pseudopotentials\": pseudopotentials, \n            \"kpts\": (3, 3, 3),\n            \"calculation\": \"scf\",\n            \"smearing\": 0.02,\n        },\n        executable_str=\"mpirun -np 1 pw.x -in input.pwi > output.pwo\",\n        delayed=True,\n        output_file_lst=[],\n        output_key_lst=[\"energy\", \"volume\"],\n    )\n    energy_lst.append(job_strain.output.energy)\n    volume_lst.append(job_strain.output.volume)","metadata":{"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"As the quantum espresso calculations are the computationally expensive steps they are combined in a python function to be submitted to dedicated computing resources. In contrast the creation of the atomistic structure and the plotting of the energy volume curve are executed in the users process.\n\nThe remaining simulation protocol, can be summarized in a few lines. The required modules are imported, a `Project` object is created which represents a folder on the filesystem, the `wrap_python_function()` is used to convert the computationally expensive steps of the workflow into a single `Job` object and the resulting energy volume curve is plotted: ","metadata":{}},{"cell_type":"code","source":"def collect(volume_lst, energy_lst):\n    return {\"volume\": volume_lst, \"energy\": energy_lst}","metadata":{"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"results = project.wrap_python_function(\n    python_function=collect,\n    volume_lst=volume_lst,\n    energy_lst=energy_lst,\n    delayed=True,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"result_dict = results.pull()\nresult_dict","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"The job job_qe_minimize was saved and received the ID: 1\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def plot_energy_volume_curve(volume_lst, energy_lst):\n    plt.plot(volume_lst, energy_lst)\n    plt.xlabel(\"Volume\")\n    plt.ylabel(\"Energy\")\n    plt.savefig(\"evcurve.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This concludes the first version of the simulation workflow, in the following the submission to HPC resources, the different options for data storage and the publication of the workflow are briefly discussed.","metadata":{}},{"cell_type":"code","source":"plot_energy_volume_curve(\n    volume_lst=result_dict[\"volume\"], \n    energy_lst=result_dict[\"energy\"]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission to an HPC / Check pointing / Error handling\nWhile the local installation of the `pyiron_base` workflow manager requires no additional configuration, the connection to an HPC system is more evolved. The existing examples provided for specific HPC systems can be converted to jinja2 templates, by defining variables with double curly brackets. A minimalist template could be: \n```\n#!/bin/bash\n#SBATCH --job-name={{job_name}}\n#SBATCH --chdir={{working_directory}}\n#SBATCH --cpus-per-task={{cores}}\n\n{{command}}\n```\nHere the `job_name`, the `working_directory` and the number of compute `cores` can be specified as parameters. In the `pyiron_base` workflow manager such a submission script can then be selected based on its name as parameter of the `server` object:\n```python\njob_workflow.server.queue = \"my_queue\"\njob_workflow.server.cores = 64\n```\nThese lines are inserted before calling the `run()` function. The rest of the simulation protocol remains the same.\n\nWhen simulation protocols are up-scaling and iterated over a large number of parameters, certain parameter combinations might lead to poor conversion or even cause simulation code crashes. In the `pyiron_base` workflow manager these calculation are marked as `aborted`. This gives the user to inspect the calculation and in case the crash was not related to the parameter combination, individual jobs can be removed with the `remove_job()` function. Afterwards, the simulation protocol can be executed again. In this case the `pyiron_base` workflow manager recognizes the already completed calculation and only re-evaluates the removed broken calculation. \n\n## Data Storage / Data Sharing\nIn the `pyiron_base` workflow manager the input of the calculation as well as the output are stored in the hierachical data format (HDF). In addition, `pyiron_base` can use a Structured Query Language (SQL) database, which acts as an index of all the `Job` objects and their HDF5 files. This file-based approach allows the user easily to browse through the results and at the same time the compressed storage in HDF5 and the internal hierarchy of the data format, enable the efficient storage of large tensors, like atomistic trajectories. \n\n## Publication of the workflow\nThe `pyiron_base` workflow manager provides a publication template to publish simulation workflows on Github. This template enables both the publication of the workflow as well as the publication of the results generated with a given workflow. For reproduciblity this publication template is based on sharing a conda environment file `environment.yml` in combination with the Jupyter notebook containing the simulation protocol and the archived `Job` objects. The Jupyter notebook is then rendered as static website with links to enable interactive execution using Jupyterbook and the mybinder service. As the `pyiron_base` workflow manager reloads existing calculation from the archive, a user of the interactive mybinder environment does not have to recompute the computationally expensive steps and still has the opportunity to interact with the provided workflow and data. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
